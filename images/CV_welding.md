---
title: 'Computer vision : d√©tection de d√©fauts de soudure' 
publishDate: 2025-01-10 00:00:00
img: /assets/CV_welding.jpg
img_alt: Computer vision for detecting welding defects
description: |
tags:
  - Computer vision
  - YOLO
  - PowerBI
---

### üéØ Contexte & Objectif
#### ¬∑ Pr√©sentation du besoin m√©tier
Dans les industries critiques comme le p√©trole et le gaz, la qualit√© des soudures est primordiale pour garantir l'int√©grit√© et la s√©curit√© des infrastructures, notamment des pipelines. Les m√©thodes d'inspection manuelle sont non seulement longues et co√ªteuses, mais √©galement sujettes √† l'erreur humaine.
L'automatisation de la d√©tection de d√©fauts de soudure √† l'aide de la vision par ordinateur permet d'am√©liorer la fiabilit√©, d'acc√©l√©rer les processus de contr√¥le qualit√© et de r√©duire les co√ªts op√©rationnels.

#### ¬∑ Objectif final √† atteindre
L'objectif est d'entra√Æner et de comparer les performances de deux mod√®les de d√©tection d'objets de pointe, **YOLO** et **DETR**, pour identifier avec pr√©cision les d√©fauts de soudure. Le but est de d√©terminer lequel est le plus adapt√© √† ce cas d'usage et de visualiser les r√©sultats de mani√®re interactive grace √† PowerBI.

***

### üß∞ Donn√©es
#### ¬∑ Source des donn√©es
Le projet utilise le jeu de donn√©es public "The Welding Defect Dataset - v2", qui contient des images de soudures avec diff√©rents types de d√©fauts.
[Lien vers le dataset sur Kaggle](https://www.kaggle.com/datasets/sukmaadhiwijaya/welding-defect-object-detection/data)

#### ¬∑ Volume et structure
Le dataset est divis√© en ensembles d'entra√Ænement, de validation et de test. Les annotations (bounding boxes) sont fournies au format YOLO (.txt), o√π chaque ligne contient la classe et les coordonn√©es de la bo√Æte englobante.

#### ¬∑ Type de variables (classes)
Le mod√®le a √©t√© entra√Æn√© pour d√©tecter et classifier les soudures en trois cat√©gories :
-   `bad weld` (mauvaise soudure)
-   `good weld` (bonne soudure)
-   `defect` (d√©faut sp√©cifique)

***

### üß™ M√©thodologie
#### ¬∑ Exploration et nettoyage des donn√©es
Pour identifier les potentiels biais lors de l'entainement, les caract√©ristiques des annotations ont √©t√© structur√© dans un DataFrame pandas pour faciliter la comparaison. L'analyse s'est concentr√©e sur la r√©partition des annotations sur les images (centre, aire, etc...). 

#### ¬∑ Mod√©lisation (algorithmes, m√©triques utilis√©es)
Deux architectures de mod√®les ont √©t√© entra√Æn√©es et compar√©es :
1.  **YOLO 11n:** Un mod√®le rapide et efficace, bas√© sur une approche √† un seul passage (single-shot detector).
2.  **DETR (DEtection TRansformer) :** Un mod√®le bas√© sur une architecture de Transformer, qui traite la d√©tection d'objets comme un probl√®me de "set prediction".

#### ¬∑ Entra√Ænement des mod√®les
L‚Äôentra√Ænement des mod√®les **YOLOv11n** et **DETR** a d√©but√© par une phase de **pr√©paration des donn√©es**.  
Les images et annotations ont √©t√© nettoy√©es, format√©es (YOLO pour YOLOv11n, COCO pour DETR), puis r√©parties en trois ensembles :  
- **train**  
- **validation**  
- **test**

Les deux architectures ont ensuite √©t√© **configur√©es** avec des param√®tres adapt√©s :  
- taille des images  
- nombre de classes  
- taux d‚Äôapprentissage  
- nombre d‚Äô√©pochs  
- batch size

Chaque mod√®le a √©t√© **entra√Æn√© s√©par√©ment** sur les donn√©es d‚Äôapprentissage.

Pendant l'entra√Ænement, les performances ont √©t√© suivies √† l‚Äôaide de m√©triques telles que :  
- **loss** (perte)  
- **mAP** (mean Average Precision)  

L‚Äôensemble de validation a permis de **surveiller l‚Äôoverfitting** et d‚Äôajuster les hyperparam√®tres si n√©cessaire.

Enfin, une fois l‚Äôentra√Ænement termin√©, les mod√®les ont √©t√© **√©valu√©s sur le jeu de test** pour permettre une comparaison objective de leurs performances.

#### ¬∑ Comparaison des pr√©dictions
La comparaison est au c≈ìur du projet. Le notebook met en place une m√©thodologie rigoureuse :
1.  **Standardisation des pr√©dictions :** Les pr√©dictions des deux mod√®les sont converties et sauvegard√©es au format COCO JSON pour une √©valuation standardis√©e.
2.  **Cr√©ation d'un DataFrame comparatif :** Pour chaque bo√Æte englobante de la v√©rit√© terrain, le script identifie la pr√©diction la plus proche (bas√©e sur la distance euclidienne des centres) pour chaque mod√®l.
3.  **Exportation pour visualisation :** Un DataFrame final fusionne les informations de la v√©rit√© terrain avec les pr√©dictions correspondantes de YOLO et DETR, incluant la classe, les coordonn√©es et l'aire des bo√Ætes. Ce fichier est la source de donn√©es pour l'analyse visuelle.

#### ¬∑ Mise en application
La finalit√© de l'analyse est un tableau de bord interactif. Les donn√©es comparatives pr√©par√©es dans le notebook sont export√©es pour √™tre utilis√©es dans **Power BI**, permettant une exploration visuelle et approfondie des performances des deux mod√®les.

***

### üìä R√©sultats
#### ¬∑ Performances du ou des mod√®les

Pour fournir un premier niveau de comparaison entre YOLOv11n et DETR, les performances ont √©t√© √©valu√©es √† l‚Äôaide de **m√©triques standards en d√©tection d‚Äôobjets**. Ces mesures permettent d‚Äôanalyser la qualit√© des pr√©dictions √† diff√©rents niveaux.

- **mAP50 (mean Average Precision √† IoU 0.5)**  
  C‚Äôest la moyenne de la pr√©cision sur toutes les classes, en consid√©rant une pr√©diction correcte si l‚Äôobjet d√©tect√© recouvre au moins 50 % de l‚Äôobjet r√©el. Cette m√©trique est souvent utilis√©e comme r√©f√©rence de base.

- **mAP50:95**  
  Cette version plus stricte du mAP prend en compte plusieurs seuils de recouvrement (IoU) entre 0.5 et 0.95. Elle permet de mieux √©valuer la **qualit√© de localisation** des objets, et donne une vision plus compl√®te des performances du mod√®le.

- **Pr√©cision (Precision)**  
  Mesure la proportion de bonnes d√©tections parmi toutes les pr√©dictions faites par le mod√®le. Une haute pr√©cision signifie que le mod√®le fait peu de fausses d√©tections.

- **Rappel (Recall)**  
  √âvalue la capacit√© du mod√®le √† retrouver tous les objets pr√©sents dans l‚Äôimage. Un bon rappel signifie que le mod√®le oublie peu d‚Äôobjets.

En combinant ces m√©triques, on peut √©valuer si un mod√®le est plut√¥t conservateur (pr√©cis mais peu sensible) ou plus sensible (bon rappel mais avec plus de fausses d√©tections).  
Ces r√©sultats permettent de mieux comprendre les **forces et faiblesses** de chaque mod√®le dans diff√©rents contextes.

#### ¬∑ Comparaison des approches
L'analyse dans Power BI permet de comparer visuellement les mod√®les sur plusieurs axes :
-   Nombre de d√©fauts correctement identifi√©s.
-   Taux de fausses d√©tections.
-   Pr√©cision de la localisation des d√©fauts (comparaison des aires et des coordonn√©es des bo√Ætes).

#### ¬∑ Interpr√©tation des r√©sultats
La visualisation des donn√©es via Power BI permet de tirer des conclusions claires sur les forces et faiblesses de chaque mod√®le dans le contexte sp√©cifique de la d√©tection de d√©fauts de soudure, aidant √† choisir la meilleure approche pour un d√©ploiement industriel. Au terme de la comparaison, de meilleures performances ont √©t√© observ√©e avec Yolo, que ce soit en terme de qualit√© des pr√©dictions ou en terme de vitesse et simplicit√© d'entra√Ænement.

***

### üí° Ce que j‚Äôai appris
#### ¬∑ Comp√©tences acquises ou renforc√©es
-   Entra√Ænement et fine-tuning de mod√®les de d√©tection d'objets (YOLO, DETR).
-   Mise en place d'un pipeline de comparaison de mod√®les de Computer Vision.
-   Manipulation de donn√©es g√©ospatiales (bounding boxes) et conversion entre diff√©rents formats d'annotation (YOLO, COCO).
-   Utilisation de pandas pour structurer et fusionner des donn√©es complexes (v√©rit√© terrain et pr√©dictions multiples).
-   **Data Visualization :** Cr√©ation d'un tableau de bord avec Power BI pour pr√©senter et analyser les r√©sultats d'exp√©riences de machine learning.


